import streamlit as st
import json
import datetime
import requests
import math

def load_json_data_from_github(repo, file_path):
    """å¾ GitHub åŠ è¼‰ JSON æ•¸æ“š"""
    url = f"https://raw.githubusercontent.com/{repo}/main/{file_path}"
    response = requests.get(url)
    if response.status_code == 200:
        return json.loads(response.text)
    else:
        st.error(f"Failed to load data from GitHub. Status code: {response.status_code}")
        return None

def search_entries(data, search_term, selected_feeds):
    """æœç´¢æ‰€é¸ feed ä¸­ç¬¦åˆé—œéµå­—çš„æ¢ç›®"""
    if not search_term and not selected_feeds:
        return data
    
    result = {}
    search_term = search_term.lower() if search_term else ""
    
    for feed_name, feed_data in data.items():
        if selected_feeds and feed_name not in selected_feeds:
            continue
        
        filtered_entries = [
            entry for entry in feed_data['entries']
            if not search_term or
            search_term in entry['title'].lower() or
            search_term in entry['title_translated'].lower() or
            search_term in entry['tldr'].lower()
        ]
        
        if filtered_entries:
            result[feed_name] = {
                'feed_title': feed_data['feed_title'],
                'feed_link': feed_data['feed_link'],
                'feed_updated': feed_data['feed_updated'],
                'entries': filtered_entries
            }
    
    return result

def display_entries(data, items_per_page=10):
    """é¡¯ç¤ºæ‰€æœ‰é¸ä¸­ feed çš„æ¢ç›®ï¼Œå¸¶åˆ†é åŠŸèƒ½"""
    all_entries = []
    for feed_data in data.values():
        all_entries.extend([(entry, feed_data['feed_title']) for entry in feed_data['entries']])
    
    all_entries.sort(key=lambda x: x[0]['published'], reverse=True)
    
    total_entries = len(all_entries)
    total_pages = max(1, math.ceil(total_entries / items_per_page))

    # åˆ†é æ§ä»¶
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        page = st.number_input(f"é ç¢¼ (å…± {total_pages} é )", min_value=1, max_value=total_pages, value=1, step=1)
    
    start_idx = (page - 1) * items_per_page
    end_idx = min(start_idx + items_per_page, total_entries)
    
    st.write(f"é¡¯ç¤ºç¬¬ {start_idx + 1} åˆ° {end_idx} ç¯‡æ–‡ç« ï¼Œå…± {total_entries} ç¯‡")
    
    for entry, feed_title in all_entries[start_idx:end_idx]:
        with st.expander(f"**{entry['title']}**\n*{entry['title_translated']}* (ä¾†è‡ª: {feed_title})"):
            st.write(f"Published: {entry['published']}")
            st.markdown(entry['tldr'])
            st.markdown(f"[PubMed]({entry['link']})")

def main():
    st.set_page_config(page_title="è½åŠ›æœŸåˆŠé€Ÿå ±", page_icon="ğŸ“š", layout="wide")
    st.title("ğŸ“š è½åŠ›æœŸåˆŠé€Ÿå ±")

    github_repo = "xxcyl/rss-feed-processor"
    file_path = "rss_data_bilingual.json"
    
    data = load_json_data_from_github(github_repo, file_path)
    if data is None:
        return
    
    # å´é‚Šæ¬„ï¼šç¯©é¸å™¨
    with st.sidebar:
        st.header("ç¯©é¸å™¨")
        feed_names = list(data.keys())
        selected_feeds = st.multiselect("é¸æ“‡è¦é¡¯ç¤ºçš„ Feed", feed_names, default=feed_names)
        search_term = st.text_input("æœç´¢æ–‡ç«  (æ¨™é¡Œæˆ–æ‘˜è¦)", "")

    # ä¸»å…§å®¹å€
    filtered_data = search_entries(data, search_term, selected_feeds)
    
    if filtered_data:
        total_feeds = len(filtered_data)
        total_articles = sum(len(feed_data['entries']) for feed_data in filtered_data.values())
        st.write(f"é¡¯ç¤º {total_feeds} å€‹ feed ä¸­çš„ {total_articles} ç¯‡æ–‡ç« ")
        display_entries(filtered_data)
    else:
        st.write("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„æ–‡ç« ã€‚")

if __name__ == "__main__":
    main()
