import streamlit as st
import json
import datetime
import requests
import math
import logging

logging.basicConfig(level=logging.INFO)

def load_json_data_from_github(repo, file_path):
    """å¾ GitHub åŠ è¼‰ JSON æ•¸æ“š"""
    url = f"https://raw.githubusercontent.com/{repo}/main/{file_path}"
    response = requests.get(url)
    if response.status_code == 200:
        return json.loads(response.text)
    else:
        st.error(f"Failed to load data from GitHub. Status code: {response.status_code}")
        return None

def search_entries(data, search_term, selected_feeds):
    """æœç´¢æŒ‡å®šæœŸåˆŠä¸­ç¬¦åˆé—œéµå­—çš„æ¢ç›®"""
    result = {}
    search_term = search_term.lower() if search_term else ""
    
    for feed_name, feed_data in data.items():
        if selected_feeds and feed_name not in selected_feeds:
            continue
        
        filtered_entries = [
            entry for entry in feed_data.get('entries', [])
            if not search_term or
            search_term in entry.get('title', '').lower() or
            search_term in entry.get('title_translated', '').lower() or
            search_term in entry.get('tldr', '').lower()
        ]
        
        if filtered_entries:
            result[feed_name] = {
                'feed_title': feed_data.get('feed_title', 'Unknown Feed'),
                'feed_link': feed_data.get('feed_link', '#'),
                'feed_updated': feed_data.get('feed_updated', 'Unknown date'),
                'entries': filtered_entries
            }
    
    return result
    
def display_entries(data, journal_urls, items_per_page=10):
    """é¡¯ç¤ºæ‰€æœ‰é¸ä¸­æœŸåˆŠçš„æ¢ç›®ï¼Œå¸¶åˆ†é åŠŸèƒ½"""
    try:
        all_entries = []
        for feed_name, feed_data in data.items():
            all_entries.extend([(entry, feed_name) for entry in feed_data.get('entries', [])])
        
        all_entries.sort(key=lambda x: x[0].get('published', ''), reverse=True)
        
        total_entries = len(all_entries)
        total_pages = max(1, math.ceil(total_entries / items_per_page))

        if 'current_page' not in st.session_state:
            st.session_state.current_page = 1

        st.session_state.current_page = min(st.session_state.current_page, total_pages)

        start_idx = (st.session_state.current_page - 1) * items_per_page
        end_idx = min(start_idx + items_per_page, total_entries)
        
        entries_container = st.container()
        
        if total_entries > 0:
            with entries_container:
                for i, (entry, feed_name) in enumerate(all_entries[start_idx:end_idx], start=1):
                    title = entry.get('title', 'No Title')
                    title_translated = entry.get('title_translated', 'No Translated Title')
                    unique_title = f"**{title}**\n*{title_translated}*"
                    key = f"expander_{st.session_state.current_page}_{i}"
                    logging.info(f"Creating expander with key: {key}")
                    with st.expander(label=unique_title, expanded=False, icon="ğŸ“", key=key):
                        st.write(f"ç™¼å¸ƒæ—¥æœŸ: {entry.get('published', 'Unknown date')}")
                        st.markdown(entry.get('tldr', 'No summary available'))
                        journal_url = journal_urls.get(feed_name, "#")
                        if journal_url != "#":
                            st.markdown(f"ğŸ”— [PubMed]({entry.get('link', '#')}) ğŸ“š [{feed_name}]({journal_url})")
                        else:
                            st.markdown(f"ğŸ”— [PubMed]({entry.get('link', '#')}) ğŸ“š {feed_name}")

            st.write("---")
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                def change_page():
                    st.session_state.current_page = st.session_state.new_page
                    st.rerun()

                st.number_input(
                    f"é ç¢¼ (å…± {total_pages} é )",
                    min_value=1,
                    max_value=total_pages,
                    value=st.session_state.current_page,
                    step=1,
                    key="new_page",
                    on_change=change_page
                )
        else:
            st.write("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„æ–‡ç« ã€‚")
    except Exception as e:
        logging.error(f"Error in display_entries: {str(e)}")
        st.error("An error occurred while displaying entries. Please try again.")

def show_introduction():
    """é¡¯ç¤ºæœ€çµ‚æ›´æ–°å¾Œçš„ç³»çµ±ä»‹ç´¹ï¼ŒåŒ…å«è­¦èª"""
    st.markdown("""
    ## ğŸŒŸ ä¸»è¦åŠŸèƒ½èˆ‡ç‰¹é»

    - ç€è¦½ä¸¦æœç´¢è½åŠ›å­¸ã€èªè¨€æ²»ç™‚åŠç›¸é—œè·¨é ˜åŸŸæœŸåˆŠçš„æœ€æ–°æ–‡ç« 
    - æœŸåˆŠåˆ†ç‚ºä¸‰é¡ï¼šè½åŠ›å­¸ã€èªè¨€æ²»ç™‚ã€æ©«è·¨å…©é¡ï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥æ‰¾
    - æä¾›è‹±æ–‡åŸæ–‡å’Œä¸­æ–‡ç¿»è­¯çš„é›™èªæ”¯æŒ
    - æ¯ç¯‡æ–‡ç« éƒ½æœ‰ AI ç”Ÿæˆçš„ä¸­æ–‡ TL;DR æ‘˜è¦
    - é¡¯ç¤ºæ¯å€‹æœŸåˆŠçš„æ–‡ç« æ•¸é‡ï¼Œå¹«åŠ©æ‚¨äº†è§£æ›´æ–°æƒ…æ³
    - æŸ¥çœ‹æ–‡ç« çš„ä¸­è‹±æ–‡æ¨™é¡Œã€ç™¼å¸ƒæ—¥æœŸå’Œä¸­æ–‡æ‘˜è¦
    - æä¾›æ¯ç¯‡ PubMed é€£çµå’ŒæœŸåˆŠå®˜æ–¹ç¶²ç«™é€£çµ
    - å®šæœŸè‡ªå‹•æ›´æ–°ï¼Œç¢ºä¿ç²å–æœ€æ–°ç ”ç©¶è³‡è¨Š
    
    ## âš ï¸ æ³¨æ„äº‹é …

    è«‹æ³¨æ„ï¼ŒAI è™•ç†ç”Ÿæˆçš„ TL;DR æ‘˜è¦å’Œä¸­æ–‡ç¿»è­¯å¯èƒ½å­˜åœ¨éŒ¯èª¤æˆ–ä¸æº–ç¢ºä¹‹è™•ã€‚ç‚ºç¢ºä¿è³‡è¨Šçš„æº–ç¢ºæ€§ï¼Œæˆ‘å€‘å¼·çƒˆå»ºè­°æ‚¨åƒè€ƒåŸæ–‡å…§å®¹ã€‚é€™äº› AI ç”Ÿæˆçš„å…§å®¹åƒ…ä¾›å¿«é€Ÿç€è¦½åƒè€ƒï¼Œä¸æ‡‰æ›¿ä»£å°åŸå§‹ç ”ç©¶è«–æ–‡çš„ä»”ç´°é–±è®€å’Œç†è§£ã€‚
    """)

def main():
    try:
        st.set_page_config(page_title="è½èªæœŸåˆŠé€Ÿå ±", page_icon="ğŸ“š", layout="wide")

        if 'current_page' not in st.session_state:
            st.session_state.current_page = 1
        if 'previous_search' not in st.session_state:
            st.session_state.previous_search = ""
        if 'previous_feeds' not in st.session_state:
            st.session_state.previous_feeds = []

        github_repo = "xxcyl/rss-feed-processor"
        file_path = "rss_data_bilingual.json"
        
        data = load_json_data_from_github(github_repo, file_path)
        if data is None:
            st.error("Failed to load data from GitHub. Please try again later.")
            return
        
        # åŠ è¼‰æœŸåˆŠé…ç½®
        try:
            with open('journal_config.json', 'r', encoding='utf-8') as f:
                journal_config = json.load(f)
        except FileNotFoundError:
            st.error("Journal configuration file not found. Please check your setup.")
            return
        except json.JSONDecodeError:
            st.error("Invalid journal configuration file. Please check the file format.")
            return
        
        # å‰µå»ºæœŸåˆŠåç¨±åˆ° URL çš„æ˜ å°„
        journal_urls = {j['name']: j['url'] for c in journal_config['categories'].values() for j in c}

        # å°‡ä¸»æ¨™é¡Œç§»åˆ° tab ä¸Šæ–¹
        st.title("ğŸ“š è½èªæœŸåˆŠé€Ÿå ±")

        tab1, tab2 = st.tabs(["ğŸ  ä¸»é ", "â„¹ï¸ ç³»çµ±ä»‹ç´¹"])
        
        with tab1:
            # æœç´¢æ¡†ç§»åˆ°ä¸»ç•«é¢æœ€ä¸Šæ–¹
            search_term = st.text_input("ğŸ” æœç´¢æ–‡ç«  (æ¨™é¡Œæˆ–æ‘˜è¦)", "", key="search_input")

            # å´é‚Šæ¬„ï¼šç¯©é¸å™¨
            with st.sidebar:
                st.subheader("æœŸåˆŠé¸æ“‡")
                
                selected_feeds = []
                
                for category, journals in journal_config['categories'].items():
                    with st.expander(f"ğŸ“‚ {category}", expanded=True):
                        for journal in journals:
                            if journal['name'] in data:
                                article_count = len(data[journal['name']]['entries'])
                                if st.checkbox(f"{journal['name']} ({article_count})", key=f"checkbox_{journal['name']}"):
                                    selected_feeds.append(journal['name'])
                
                all_categorized_journals = [j['name'] for c in journal_config['categories'].values() for j in c]
                uncategorized_journals = [feed for feed in data.keys() if feed not in all_categorized_journals]
                if uncategorized_journals:
                    st.warning(f"è­¦å‘Šï¼šä»¥ä¸‹æœŸåˆŠæœªè¢«åˆ†é¡ï¼š{', '.join(uncategorized_journals)}")

            # ä¸»å…§å®¹å€
            if search_term != st.session_state.previous_search or selected_feeds != st.session_state.previous_feeds:
                st.session_state.current_page = 1
                st.session_state.previous_search = search_term
                st.session_state.previous_feeds = selected_feeds

            filtered_data = search_entries(data, search_term, selected_feeds if selected_feeds else None)
            
            if filtered_data:
                total_feeds = len(filtered_data)
                total_articles = sum(len(feed_data['entries']) for feed_data in filtered_data.values())
                
                st.write(f"ğŸ“Š é¡¯ç¤º {total_feeds} å€‹æœŸåˆŠä¸­çš„ {total_articles} ç¯‡æ–‡ç« ")              
                display_entries(filtered_data, journal_urls)
            else:
                st.write("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„æ–‡ç« ã€‚")
        
        with tab2:
            show_introduction()
    except Exception as e:
        logging.error(f"Error in main function: {str(e)}")
        st.error("An unexpected error occurred. Please try again later.")

if __name__ == "__main__":
    main()
