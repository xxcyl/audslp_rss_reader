import streamlit as st
import json
import datetime
import requests
import math
import re

def load_json_data_from_github(repo, file_path):
    """å¾ GitHub åŠ è¼‰ JSON æ•¸æ“š"""
    url = f"https://raw.githubusercontent.com/{repo}/main/{file_path}"
    response = requests.get(url)
    if response.status_code == 200:
        return json.loads(response.text)
    else:
        st.error(f"Failed to load data from GitHub. Status code: {response.status_code}")
        return None

def search_entries(data, search_term):
    """æœç´¢æ‰€æœ‰ feed ä¸­ç¬¦åˆé—œéµå­—çš„æ¢ç›®"""
    if not search_term:
        return data
    
    search_term = search_term.lower()
    result = {}
    for feed_name, feed_data in data.items():
        filtered_entries = [
            entry for entry in feed_data['entries']
            if search_term in entry['title'].lower() or
               search_term in entry['title_translated'].lower() or
               search_term in entry['tldr'].lower()
        ]
        if filtered_entries:
            result[feed_name] = {
                'feed_title': feed_data['feed_title'],
                'feed_link': feed_data['feed_link'],
                'feed_updated': feed_data['feed_updated'],
                'entries': filtered_entries
            }
    return result

def display_feed(feed_data, feed_name, items_per_page=10):
    """é¡¯ç¤ºå–®å€‹ feed çš„å…§å®¹ï¼Œå¸¶åˆ†é åŠŸèƒ½"""
    st.write(f"Last updated: {feed_data['feed_updated']}")
    
    total_entries = len(feed_data['entries'])
    total_pages = max(1, math.ceil(total_entries / items_per_page))

    # å…ˆå‰µå»ºä¸€å€‹ç©ºçš„ä½”ä½ç¬¦ä¾†é¡¯ç¤ºåˆ†é æ§ä»¶
    paging_placeholder = st.empty()
    
    # é¡¯ç¤ºå…§å®¹
    entries_placeholder = st.empty()
    
    # åœ¨åº•éƒ¨å‰µå»ºåˆ†é æ§ä»¶
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        page = st.number_input(f"é ç¢¼ (å…± {total_pages} é )", min_value=1, max_value=total_pages, value=1, step=1, key=f"{feed_name}_page")
    
    start_idx = (page - 1) * items_per_page
    end_idx = min(start_idx + items_per_page, total_entries)
    
    # ä½¿ç”¨ entries_placeholder é¡¯ç¤ºå…§å®¹
    with entries_placeholder.container():
        for entry in feed_data['entries'][start_idx:end_idx]:
            with st.expander(f"**{entry['title']}**\n*{entry['title_translated']}*"):
                st.write(f"Published: {entry['published']}")
                st.markdown(entry['tldr'])
                st.markdown(f"[PubMed]({entry['link']})")
    
    # ä½¿ç”¨ paging_placeholder åœ¨é ‚éƒ¨é¡¯ç¤ºç•¶å‰é ç¢¼ä¿¡æ¯
    paging_placeholder.write(f"ç•¶å‰é é¢: {page} / {total_pages}")

def main():
    st.set_page_config(page_title="è½åŠ›æœŸåˆŠé€Ÿå ±", page_icon="ğŸ“š")
    st.title("ğŸ“š è½åŠ›æœŸåˆŠé€Ÿå ±")

    github_repo = "xxcyl/rss-feed-processor"
    file_path = "rss_data_bilingual.json"
    
    data = load_json_data_from_github(github_repo, file_path)
    if data is None:
        return
    
    # æ·»åŠ æœç´¢æ¡†
    search_term = st.text_input("æœç´¢æ‰€æœ‰ feed ä¸­çš„æ–‡ç«  (æ¨™é¡Œæˆ–æ‘˜è¦)", "")
    
    # åŸ·è¡Œæœç´¢
    filtered_data = search_entries(data, search_term)
    
    # å‰µå»ºä¸‹æ‹‰å¼é¸å–®
    feed_names = list(filtered_data.keys())
    if search_term:
        st.write(f"æœç´¢çµæœ: åœ¨ {len(feed_names)} å€‹ feed ä¸­æ‰¾åˆ°ç›¸é—œæ–‡ç« ")
    else:
        st.write(f"å…±æœ‰ {len(feed_names)} å€‹ RSS feed å¯ä¾›é¸æ“‡")
    
    if feed_names:
        selected_feed = st.selectbox("é¸æ“‡ RSS Feed", feed_names)
        
        # é¡¯ç¤ºé¸ä¸­çš„ feed
        st.header(selected_feed)
        display_feed(filtered_data[selected_feed], selected_feed)
    else:
        st.write("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæœç´¢æ¢ä»¶çš„æ–‡ç« ã€‚")

if __name__ == "__main__":
    main()
